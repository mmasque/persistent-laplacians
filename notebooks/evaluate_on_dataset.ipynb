{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can the spectral gap of a filtration predict whether a dataset is a circle or an infinity sign? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I assess whether the smallest nonzero eigenvalues obtained from a filtration of point-cloud data can be used to classify it. \n",
    "The goal is to distinguish between pointclouds of $S^1$ and of $S^1 \\vee S^1$. \n",
    "\n",
    "1. For each of the two spaces, 50 datasets with 50 noisy points in each are generated. \n",
    "2. A filtration is generated from the data using Gudhi's implementation of Alpha complexes.\n",
    "3. 20 uniformly spaced indices $I$ in the filtration are selected, and for each of the approximately 200 simplicial complex pairs $(i, j) \\in I^2$ the smallest nonzero eigenvalue $\\lambda^q_{i,j}$ of the persistent Laplacian is computed at each simplicial complex dimension $q$. \n",
    "4. A Logistic regression model is trained using the nonpersistent eigenvalues in dimension 1: $(\\lambda^1_{i,i})_{i \\in I}$. This is the _non-persistent_ model.\n",
    "5. A Logistic regression model is trained using the eigenvalues in dimension 1: $(\\lambda^1_{i,j})_{(i,j) \\in I^2}$. This is the _persistent_ model.\n",
    "6. A paired t-test is run to assess whether the persistent model performs differently than the non-persistent model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tadasets\n",
    "dataset = [(tadasets.dsphere(n=50, d=1, r=2, noise=0.6, seed=i), 0) for i in range(50)] + [(tadasets.infty_sign(n=50, noise=0.6, seed=i), 1) for i in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_cross_validation(\n",
    "    dataset,\n",
    "    feature_extractor,\n",
    "    classifier=None,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs k-fold cross-validation on given data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list of (data, label) tuples\n",
    "    feature_extractor : function mapping data -> feature vector (1D numpy array)\n",
    "    classifier : sklearn-like classifier (implements fit & predict). Defaults to LogisticRegression.\n",
    "    n_splits : int, number of folds\n",
    "    random_state : int, seed for shuffling\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracies : list of float accuracy scores per fold\n",
    "    \"\"\"\n",
    "    # Build feature matrix X and label vector y\n",
    "    features = [feature_extractor(data) for data, _ in dataset]\n",
    "    # Determine max feature length\n",
    "    max_len = max(f.shape[0] for f in features)\n",
    "    # Pad features to uniform length\n",
    "    features_padded = [np.pad(f, (0, max_len - f.shape[0]), mode='constant') for f in features]\n",
    "    X = np.vstack(features_padded)\n",
    "    y = np.array([label for _, label in dataset])\n",
    "\n",
    "    # Default classifier\n",
    "    if classifier is None:\n",
    "        classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    accuracies = []\n",
    "\n",
    "    # Fold loop\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Standardize features\n",
    "        mu = X_train.mean(axis=0)\n",
    "        sigma = X_train.std(axis=0)\n",
    "        X_train_std = (X_train - mu) / sigma\n",
    "        X_test_std = (X_test - mu) / sigma\n",
    "\n",
    "        # Train & evaluate\n",
    "        clf = classifier\n",
    "        clf.fit(X_train_std, y_train)\n",
    "        preds = clf.predict(X_test_std)\n",
    "        accuracies.append(accuracy_score(y_test, preds))\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonpersistent smallest nonzero eigenvalue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from persistent_laplacians.eigenvalues import compute_eigenvalues\n",
    "def extract_nonpersistent_feature(data):\n",
    "    result = compute_eigenvalues(\n",
    "        data,\n",
    "        num_indices=20,\n",
    "        use_scipy=True,\n",
    "        use_stepwise_schur=False,\n",
    "        zero_tol=1e-6\n",
    "    )\n",
    "    # Filter result to nonpersistent dim 1 features\n",
    "    nonpersistent_dim1 = [\n",
    "        (k[0], v)\n",
    "        for k, v in result[1].items()\n",
    "        if k[0] == k[1]\n",
    "    ]\n",
    "    nonpersistent_dim1.sort(key=lambda x: x[0])\n",
    "    # Return first element of each or zero if missing\n",
    "    return np.array([vec[0] if vec else 0 for _, vec in nonpersistent_dim1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcelmasque/University/other/persistent_laplacians/python/persistent_laplacians/eigenvalues.py:27: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  return pl.smallest_eigenvalue(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracies: [1.0, 0.95, 0.9, 0.7, 0.9]\n",
      "Mean accuracy: 0.890 ± 0.102\n"
     ]
    }
   ],
   "source": [
    "accuracies_nonpersistent = run_cross_validation(\n",
    "    dataset=dataset,\n",
    "    feature_extractor=extract_nonpersistent_feature,\n",
    "    classifier=LogisticRegression(),\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mean_acc = np.mean(accuracies_nonpersistent)\n",
    "std_acc = np.std(accuracies_nonpersistent)\n",
    "print(f\"Cross-validated accuracies: {accuracies_nonpersistent}\")\n",
    "print(f\"Mean accuracy: {mean_acc:.3f} ± {std_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent smallest eigenvalue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from persistent_laplacians.eigenvalues import compute_eigenvalues\n",
    "def extract_persistent_feature(data):\n",
    "    result = compute_eigenvalues(\n",
    "        data,\n",
    "        num_indices=20,\n",
    "        use_scipy=True,\n",
    "        use_stepwise_schur=False,\n",
    "        zero_tol=1e-6\n",
    "    )\n",
    "    dim1_result = [x for x in result[1].items()]\n",
    "    dim1_result.sort(key=lambda x: x[1])\n",
    "    dim1_result.sort(key=lambda x: x[0])\n",
    "    # Return first element of each or zero if missing\n",
    "    return np.array([vec[0] if vec else 0 for _, vec in dim1_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcelmasque/University/other/persistent_laplacians/python/persistent_laplacians/eigenvalues.py:27: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  return pl.smallest_eigenvalue(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracies: [0.95, 0.95, 0.85, 0.7, 0.9]\n",
      "Mean accuracy: 0.870 ± 0.093\n"
     ]
    }
   ],
   "source": [
    "accuracies_persistent = run_cross_validation(\n",
    "    dataset=dataset,\n",
    "    feature_extractor=extract_persistent_feature,\n",
    "    classifier=LogisticRegression(),\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mean_acc = np.mean(accuracies_persistent)\n",
    "std_acc = np.std(accuracies_persistent)\n",
    "print(f\"Cross-validated accuracies: {accuracies_persistent}\")\n",
    "print(f\"Mean accuracy: {mean_acc:.3f} ± {std_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paired t-test p = 0.178\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel \n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_val = ttest_rel(accuracies_persistent, accuracies_nonpersistent)\n",
    "print(f\"paired t-test p = {p_val:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
